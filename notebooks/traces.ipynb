{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from rich.progress import Progress\n",
    "import re\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from networkx.drawing.nx_agraph import graphviz_layout\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from collections import deque\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p data && cd data && wget -c --retry-connrefused --tries=0 --timeout=50 http://aliopentrace.oss-cn-beijing.aliyuncs.com/v2018Traces/batch_task.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd data && tar -xvzf batch_task.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://github.com/alibaba/clusterdata/blob/master/cluster-trace-v2018/fetchData.sh\n",
    "df = pd.read_csv('data/batch_task.csv', names=['task_name', 'instance_num', 'job_name', 'task_type', 'status', 'start_time', 'end_time', 'plan_cpu', 'plan_mem'])\n",
    "df['duration'] = df['end_time'] - df['start_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.head(n=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task name is:\n",
    "# a) containing dependencies (like 'J4_2_3' -> task 4 depends on 2 and 3)\n",
    "TASK_NAME_RE = re.compile('^[^_]*[A-Z](?P<task_id>\\d+)(_(?P<deps>[\\d+_]+))?(_Stg\\d+)?$') # Note: sometimes job ends with _Stg*\n",
    "# b) independent task (like 'task_LTE4NjUxMjg5NDY5MDI4NjAzNzU=')\n",
    "SINGLE_TASK_RE = re.compile('^task_[a-zA-Z0-9]+=*$')\n",
    "# c) 'MergeTask'\n",
    "\n",
    "# check that we cover all cases:\n",
    "assert df.task_name.apply(lambda f: TASK_NAME_RE.match(f) is not None or SINGLE_TASK_RE.match(f) is not None or f == 'MergeTask').all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that 'MergeTask's are in fact independent tasks (i.e. the only task in a job)\n",
    "def get_merge_task_stats(df):\n",
    "    df = df[['job_name', 'task_name']].copy()\n",
    "    df['is_merge_task'] = df['task_name'] == 'MergeTask'\n",
    "    return df.groupby('job_name').agg(\n",
    "        count=pd.NamedAgg('task_name', 'count'),\n",
    "        mergeCount=pd.NamedAgg('is_merge_task', 'sum')\n",
    "    )\n",
    "\n",
    "assert len(get_merge_task_stats(df).query('mergeCount > 0 and count > 1')) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract dependency info\n",
    "def get_task_index_and_deps(row):\n",
    "    task_name = row.task_name\n",
    "    if m := TASK_NAME_RE.match(task_name):\n",
    "        if m.group('deps'):\n",
    "            deps = [int(item) for item in m.group('deps').split('_') if item != '']\n",
    "        else:\n",
    "            deps = []\n",
    "        return int(m.group('task_id')), deps\n",
    "    else:\n",
    "        return 1, []\n",
    "\n",
    "df[['task_index', 'task_deps']] = df[['task_name']].apply(get_task_index_and_deps, result_type='expand', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['job_name', 'task_index', 'task_deps', 'duration', 'instance_num']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample job\n",
    "df[['job_name', 'task_index', 'task_deps', 'duration', 'instance_num']].query('job_name == \"j_3\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter jobs with 10 or more tasks\n",
    "jobs = df.groupby(\"job_name\").filter(lambda x: len(x) >= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter jobs that are uninteresting, such as simple lines or parallel dag covered by other experiments\n",
    "\n",
    "grouped = jobs.groupby(\"job_name\")\n",
    "filtered = []\n",
    "\n",
    "with Progress() as progress:\n",
    "    task = progress.add_task(\"[red]Removing simple DAGs\", total=len(grouped))\n",
    "    for group_name, df_group in grouped:\n",
    "        progress.update(task, advance=1)\n",
    "\n",
    "        complex_deps = False\n",
    "        for row_index, row in df_group.iterrows():\n",
    "            if len(row[\"task_deps\"]) > 1:\n",
    "                complex_deps = True\n",
    "\n",
    "        if complex_deps:\n",
    "            filtered.append(df_group)\n",
    "            \n",
    "jobs = pd.concat(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_names = jobs[\"job_name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a sample\n",
    "sample_job_names = pd.DataFrame({\"job_name\": job_names.sample(n=30, random_state=1337)})\n",
    "sample_jobs = jobs[jobs[\"job_name\"].isin(sample_job_names[\"job_name\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframe to a file for cutting down on re-processing times\n",
    "jobs_cache_location = \"data/jobs_cache\"\n",
    "jobs_names_cache_location = \"data/jobs_names_cache\" \n",
    "\n",
    "if not Path(jobs_cache_location).is_file() or not Path(jobs_names_cache_location).is_file():\n",
    "    sample_jobs.to_pickle(jobs_cache_location)\n",
    "    sample_job_names.to_pickle(jobs_names_cache_location)\n",
    "    \n",
    "sample_jobs = pd.read_pickle(jobs_cache_location)\n",
    "sample_job_names = pd.read_pickle(jobs_names_cache_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_jobs[\"job_name\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_jobs.query('job_name == \"j_3302772\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure each job takes no more than 60 seconds\n",
    "\n",
    "MAX_TIME = 60\n",
    "\n",
    "for index, row in sample_jobs.iterrows():\n",
    "    sample_jobs.loc[index, 'original_duration'] = row[\"duration\"]\n",
    "    if row[\"duration\"] > MAX_TIME:\n",
    "        print(f\"scoping down {row['job_name']} task {row['task_name']} from {row['duration']}\")\n",
    "        sample_jobs.loc[index, 'duration'] = MAX_TIME\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_jobs.query('job_name == \"j_3302772\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the DAGs\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def build_graph(dag):\n",
    "    graph = nx.DiGraph()\n",
    "    dependencies = []\n",
    "    for _, task_data in dag.iterrows():\n",
    "        graph.add_node(task_data[\"task_name\"], duration=task_data[\"duration\"])\n",
    "        unique_deps = set(task_data[\"task_deps\"])\n",
    "        for dependency_index in unique_deps:\n",
    "            for _, dependency in dag.iterrows():\n",
    "                if dependency[\"task_index\"] == dependency_index:\n",
    "                    dependencies.append((dependency[\"task_name\"], task_data[\"task_name\"]))\n",
    "    graph.add_edges_from(dependencies)\n",
    "    return graph\n",
    "\n",
    "\n",
    "def critical_path(graph):\n",
    "    distance = defaultdict(lambda: 0)\n",
    "    indegree = defaultdict(lambda: 0)\n",
    "    critical_path = 0\n",
    "    queue = deque()\n",
    "    \n",
    "    for f, t in graph.edges():\n",
    "        indegree[t] += 1\n",
    "        \n",
    "    for node in graph.nodes():\n",
    "        if indegree[node] == 0:\n",
    "            queue.append(node)\n",
    "            distance[node] = graph.nodes[node][\"duration\"]\n",
    "    \n",
    "    while len(queue) != 0:\n",
    "        top = queue.popleft()\n",
    "        for f, t in graph.edges(top):\n",
    "            assert f == top\n",
    "            \n",
    "            indegree[t] -= 1\n",
    "            node_data = graph.nodes(top)\n",
    "            distance[t] = max(distance[t], distance[top] + graph.nodes[t][\"duration\"])\n",
    "            \n",
    "            if indegree[t] == 0:\n",
    "                queue.append(t)\n",
    "    \n",
    "    for _, value in distance.items():\n",
    "        if value > critical_path:\n",
    "            critical_path = value\n",
    "    \n",
    "    return critical_path\n",
    "\n",
    "\n",
    "def total_work(graph):\n",
    "    work = 0\n",
    "    \n",
    "    for node in graph.nodes():\n",
    "        work += graph.nodes[node][\"duration\"]\n",
    "        \n",
    "    return work\n",
    "\n",
    "\n",
    "def adjusted_tasks(dag):\n",
    "    changes = 0\n",
    "    for _, task_data in dag.iterrows():\n",
    "        if task_data[\"duration\"] != task_data[\"original_duration\"]:\n",
    "            changes += 1\n",
    "    return changes\n",
    "\n",
    "\n",
    "def draw(graph, name):\n",
    "    plt.figure(name)\n",
    "    plt.title(name)\n",
    "    mapping = {}\n",
    "    for node in graph.nodes():\n",
    "        mapping[node] = graph.nodes[node][\"duration\"]\n",
    "    pos = graphviz_layout(graph, prog='dot')  \n",
    "    nx.draw(\n",
    "        graph,\n",
    "        pos,\n",
    "        with_labels=False,\n",
    "        node_size=150,\n",
    "        node_color=\"#000000\",\n",
    "        width=0.8,\n",
    "        font_size=14,\n",
    "    )\n",
    "    nx.draw_networkx_labels(graph, pos, mapping, font_size=8, font_color=\"whitesmoke\")\n",
    "\n",
    "\n",
    "job_info = pd.DataFrame(columns=['job_name', 'critical_path'])\n",
    "\n",
    "for job_name in sample_job_names[\"job_name\"]:\n",
    "    job_data = sample_jobs[sample_jobs[\"job_name\"] == job_name].copy()\n",
    "    graph = build_graph(job_data)\n",
    "    \n",
    "    info = pd.DataFrame({\n",
    "        'job_name': job_name,\n",
    "        'critical_path': critical_path(graph),\n",
    "        'tasks': len(job_data),\n",
    "        'total_work': total_work(graph),\n",
    "        'adjusted_tasks': adjusted_tasks(job_data),\n",
    "    }, index=[0])\n",
    "    job_info = pd.concat([job_info, info], ignore_index=True)\n",
    "    \n",
    "    draw(graph, job_name)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_info.sort_values('tasks', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to Airflow's format\n",
    "\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "\n",
    "MAX_DURATION = 60\n",
    "INDENT = \"    \"\n",
    "DIRECTORY = \"data/generated_dags\"\n",
    "\n",
    "def task_template(task_data, job_data) -> (str, [str]):\n",
    "    dependencies = []\n",
    "    base_task = f\"\"\"\n",
    "task_{task_data['task_name']} = BashOperator(\n",
    "    task_id='{task_data['task_name']}',\n",
    "    bash_command='sleep {min(task_data['duration'], MAX_DURATION)}',\n",
    ")\n",
    "    \"\"\".strip()\n",
    "    \n",
    "    unique_deps = set(task_data[\"task_deps\"])\n",
    "    for dependency_index in unique_deps:\n",
    "        dependency = None\n",
    "        for _, task in job_data.iterrows():\n",
    "            if task[\"task_index\"] == dependency_index:\n",
    "                dependency = task\n",
    "                break\n",
    "        assert dependency is not None\n",
    "        \n",
    "        dependency_template = f\"\"\"\n",
    "task_{dependency['task_name']} >> task_{task_data['task_name']}\n",
    "\"\"\".strip()\n",
    "        dependencies.append(dependency_template)\n",
    "    \n",
    "    return base_task, dependencies\n",
    "\n",
    "for job_name in sample_job_names[\"job_name\"]:\n",
    "    job_data = sample_jobs[sample_jobs[\"job_name\"] == job_name].copy()\n",
    "    info = job_info[job_info[\"job_name\"] == job_name].copy().iloc[0]\n",
    "    schedule = 5 if info[\"critical_path\"] < 201 else 10\n",
    "    \n",
    "    \n",
    "    imports = f\"\"\"\n",
    "import pendulum\n",
    "from airflow import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "\n",
    "\n",
    "with DAG(\n",
    "    dag_id='{job_name}',\n",
    "    schedule_interval='*/{schedule} * * * *',\n",
    "    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n",
    "    catchup=False,\n",
    ") as dag:\n",
    "\"\"\".strip()\n",
    "    \n",
    "    templated_tasks = []\n",
    "    templated_dependencies = []\n",
    "    \n",
    "    for _, task_data in job_data.iterrows():\n",
    "        task_templated, dependencies_templated = task_template(task_data, job_data)\n",
    "        templated_tasks.append(task_templated)\n",
    "        templated_dependencies.extend(dependencies_templated)\n",
    "    \n",
    "    Path(DIRECTORY).mkdir(parents=True, exist_ok=True)\n",
    "    Path(f'{DIRECTORY}/{job_name}').mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    with open(f'{DIRECTORY}/{job_name}.py', 'w+') as f:\n",
    "        f.write(imports)\n",
    "        \n",
    "        task_lines = \"\\n\".join(templated_tasks)\n",
    "        tasks_data = [f\"{INDENT}{line}\" for line in task_lines.split(\"\\n\")]\n",
    "        tasks = \"\\n\".join(tasks_data)\n",
    "        \n",
    "        f.write(\"\\n\")\n",
    "        f.write(tasks)\n",
    "        \n",
    "        dependencies_lines = \"\\n\".join(templated_dependencies)\n",
    "        dependencies_data = [f\"{INDENT}{line}\" for line in dependencies_lines.split(\"\\n\")]\n",
    "        deps = \"\\n\".join(dependencies_data)\n",
    "        \n",
    "        f.write(\"\\n\")\n",
    "        f.write(deps)\n",
    "        f.write(\"\\n\")\n",
    "    \n",
    "    shutil.copyfile(f'{DIRECTORY}/{job_name}.py', f'{DIRECTORY}/{job_name}/{job_name}.py')\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get beeflow experiments config\n",
    "    \n",
    "experiments = []\n",
    "\n",
    "for job_name in job_info.sort_values('tasks', ascending=False)[\"job_name\"]:\n",
    "    experiments.append(\n",
    "    {\n",
    "        \"dags_local_path\": f'notebooks/{DIRECTORY}/{job_name}',\n",
    "        \"dag_ids\": [job_name],\n",
    "        \"metrics_collection_time_seconds\": 3600,\n",
    "        \"experiment_id\": job_name,    \n",
    "    })\n",
    "    \n",
    "print(json.dumps(experiments, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
